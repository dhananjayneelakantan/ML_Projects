{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dhananjayneelakantan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Val f1: 0.670 +- 0.081\n",
      "Logisitic Regression SGD Val f1: 0.622 +- 0.033\n",
      "SVM Huber Val f1: 0.610 +- 0.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/dhananjayneelakantan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pdftotext\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(400)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "\n",
    "df=pd.read_csv(\"Targetnames.csv\")\n",
    "#df[\"Text\"]=df\n",
    "#df['Target']=1\n",
    "#df = df.drop(df.columns[0], axis=1)\n",
    "#df.tail()\n",
    "#pdf[0]\n",
    "\n",
    "# df.head()\n",
    "\n",
    "processed_docs = []\n",
    "\n",
    "def fileread(df):\n",
    "    with open(df, \"rb\") as f:\n",
    "        conv = pdftotext.PDF(f)\n",
    "        text=conv[0]\n",
    "        processedtext = preprocess(text)\n",
    "    return processedtext\n",
    "\n",
    "\n",
    "# with open('Filewrite.txt', 'w') as f:\n",
    "#     f.write(\"\\n\\n\".join(pdf))\n",
    "\n",
    "# text[0]\n",
    "\n",
    "for i in range(len(df)):\n",
    "    dictfile = fileread(df.Path[i])\n",
    "    processed_docs.append(dictfile)\n",
    "\n",
    "#dictfile\n",
    "#processed_docs[13]\n",
    "\n",
    "# Appendframe = df\n",
    "# # #Appendframe.tail()\n",
    "# # for i in df.Text:\n",
    "# Appendframe.Text[0]=Appendframe.append(Appendframe, ignore_index=True)\n",
    "# #Appendframe = Appendframe.append(df, ignore_index=True)\n",
    "# Appendframe.Text[0]\n",
    "# ddf['Target']=\"COI\"\n",
    "# ddf['Text']=ddf['DATE (MM/DD/YYYY)']\n",
    "# fileread(df.Path[13])\n",
    "# processed_docs.to_csv(\"A.csv\")\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "# dictionary[412]\n",
    "bow_corpus =[dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "#bow_corpus[1187]\n",
    "# document_num = 0\n",
    "# bow_doc_x = bow_corpus[document_num]\n",
    "\n",
    "# for i in range(len(bow_doc_x)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_x[i][0], \n",
    "#                                                      dictionary[bow_doc_x[i][0]], \n",
    "#                                                      bow_doc_x[i][1]))\n",
    "\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 14, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "lda_model.save('lda_train.model')\n",
    "\n",
    "# for idx, topic in lda_model.print_topics(-1):\n",
    "#     print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n",
    "#                   'would','really','like','great','service','came','got'])\n",
    "\n",
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def bigrams(words, bi_min=15, tri_min=10):\n",
    "#     bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "#     bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#     return bigram_mod\n",
    "    \n",
    "# def get_corpus(df):\n",
    "#     #df['Text'] = strip_newline(df.Path)\n",
    "#     #words = list(df)\n",
    "#     words = remove_stopwords(df)\n",
    "#     bigram_mod = bigrams(words)\n",
    "#     bigram = [bigram_mod[review] for review in words]\n",
    "#     id2word = gensim.corpora.Dictionary(bigram)\n",
    "#     id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "#     id2word.compactify()\n",
    "#     corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "#     return corpus, id2word, bigram\n",
    "\n",
    "# train_corpus, train_id2word, bigram_train = get_corpus(processed_docs)\n",
    "\n",
    "# import gensim\n",
    "# import logging # This allows for seeing if the model converges. A log file is created.\n",
    "# logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter('ignore')\n",
    "#     lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "#                            corpus=train_corpus,\n",
    "#                            num_topics=1,\n",
    "#                            id2word=train_id2word,\n",
    "#                            chunksize=100,\n",
    "#                            workers=7, # Num. Processing Cores - 1\n",
    "#                            passes=50,\n",
    "#                            eval_every = 1,\n",
    "#                            per_word_topics=True)\n",
    "#     lda_train.save('lda_train.model')\n",
    "\n",
    "lda_model.print_topics(20,num_words=15)[:2]\n",
    "\n",
    "train_vecs = []\n",
    "for i in range(len(df)):\n",
    "    top_topics = lda_model.get_document_topics(bow_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(14)]\n",
    "    #topic_vec.extend([df.iloc[i].real_counts]) # counts of reviews for restaurant\n",
    "    #topic_vec.extend([len(df.iloc[i].text)]) # length review\n",
    "    train_vecs.append(topic_vec)\n",
    "    \n",
    "#train_vecs[13]\n",
    "# df.Target\n",
    "\n",
    "X = np.array(train_vecs)\n",
    "y = np.array(df.Target)\n",
    "\n",
    "kf = KFold(2, shuffle=True, random_state=42)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    # Assign CV IDX\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val_scale)\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='macro'))\n",
    "    \n",
    "    # Logistic Regression SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='macro'))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='macro'))\n",
    "\n",
    "# print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "# print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "# print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')\n",
    "\n",
    "\n",
    "\n",
    "#TESTING STARTS HERE\n",
    "dftest=pd.read_csv(\"Testnames.csv\")\n",
    "testfile = []\n",
    "for i in range(len(dftest)):\n",
    "    dictfile = fileread(dftest.Path[i])\n",
    "    testfile.append(dictfile)\n",
    "    \n",
    "\n",
    "#test['Text']=test\n",
    "#test['Target']=\"COI\"\n",
    "#test = test.drop(test.columns[0], axis=1)\n",
    "#test.tail()\n",
    "#testfile[0]\n",
    "test_corpus = [dictionary.doc2bow(doc) for doc in testfile]\n",
    "#test_corpus[0]\n",
    "##Testing\n",
    "\n",
    "# def get_bigram(df):\n",
    "#     \"\"\"\n",
    "#     For the test data we only need the bigram data built on 2017 reviews,\n",
    "#     as we'll use the 2016 id2word mappings. This is a requirement due to \n",
    "#     the shapes Gensim functions expect in the test-vector transformation below.\n",
    "#     With both these in hand, we can make the test corpus.\n",
    "#     \"\"\"\n",
    "#     #df['text'] = strip_newline(df.text)\n",
    "#     words = list(df)\n",
    "#     words = remove_stopwords(words)\n",
    "#     bigram = bigrams(words)\n",
    "#     bigram = [bigram[review] for review in words]\n",
    "#     return bigram\n",
    "\n",
    "  \n",
    "# bigram_test = get_bigram(test.Text)\n",
    "\n",
    "# test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
    "\n",
    "test_vecs = []\n",
    "for i in range(len(dftest)):\n",
    "    top_topics = lda_model.get_document_topics(test_corpus[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(14)]\n",
    "    #topic_vec.extend([df.iloc[i].real_counts]) # counts of reviews for restaurant\n",
    "    #topic_vec.extend([len(df.iloc[i].text)]) # length review\n",
    "    test_vecs.append(topic_vec)\n",
    "    \n",
    "test_vecs\n",
    "W=np.array(test_vecs)\n",
    "test_val_scale = scaler.transform(W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] [4] [4]\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "print(sgd.predict(test_val_scale), lr.predict(test_val_scale),sgd_huber.predict(test_val_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
