{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to support python2 and python3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "bias = False\n",
    "\n",
    "#Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.externals import joblib \n",
    "#to make this notebooks output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "#remove warnings\n",
    "import warnings \n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "#Variabletype Initialisation\n",
    "stringvar=['_channel','createdBy','_user.login','_transactionContext.currentSection','_validationContext','_workflow.currentState.subType','_workflow.inboxItem.assignedBy','_workflow.isError','assignTo','BatteryCount','caCity','caContactName','caZip','code','code1','consultantName','copyOver','Customer Rate Code','customer.circuit','customer.city','customer.esiid','customer.fuseDetails','customer.meterId','customer.revenueAreaCode','customer.section','customer.streetNumber','customer.zipCode','customerTitle','dba','ecName','entityType','exportPower','externalCode','interconnectionMode','noOfPhase','organization.code','ownershipInfo','parallelType','pmName','propertyOwnerCheck','reportExcessGeneration','serviceCenter','tlmHighVoltage','tlmLowVoltage','tlmPhase','tlmXfrmrType','updatedBy','workorderCode']\n",
    "floatvar=['invCapacity','inverterEfficiency','inverterQuantity','kvar','solarQuantity','tlmMeterCnt']\n",
    "datetimevar=['agreementEffectiveDate','applicationReceivedDate','applicationRejectedDate','applicationResubmittedDate','cancelledDate','createdAt','designApprovedDate','inspectionCompletedDate','inspectionRejectedDate','ptoDate','removedDate','serviceDesiredDate','StatusDate']\n",
    "\n",
    "#Leg Selection\n",
    "leg='Days_created_to_application'\n",
    "\n",
    "#Variable initialisation\n",
    "num=['invCapacity','inverterEfficiency','inverterQuantity','kvar','solarQuantity','tlmMeterCnt']\n",
    "cat=['_workflow.inboxItem.assignedBy','_user.login','organization.code','createdBy','assignTo','caCity','caZip','consultantName','customer.city','customer.zipCode','dba','ecName','pmName','updatedBy']\n",
    "\n",
    "def Train(path):\n",
    "    location = path\n",
    "    #File read and typecast\n",
    "    df = pd.read_csv(location)\n",
    "\n",
    "    for col in ['applicationReceivedDate']:\n",
    "        df[col] = df[col].astype('datetime64')\n",
    "\n",
    "    df.sort_values(by='applicationReceivedDate')\n",
    "    df = df[(df['applicationReceivedDate'] > '2019/01/01')]\n",
    "\n",
    "    #Changing dtype\n",
    "    for col in [stringvar]:\n",
    "        df[col] = df[col].astype('str')\n",
    "\n",
    "    for col in [floatvar]:\n",
    "        df[col] = df[col].astype('float64')   \n",
    "\n",
    "    for col in [datetimevar]:\n",
    "        df[col] = df[col].astype('datetime64')   \n",
    "\n",
    "    pto_df_2019 = df\n",
    "\n",
    "    \n",
    "    #Dataframe selector\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "        def __init__(self, attribute_names):\n",
    "            self.attribute_names = attribute_names\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        def transform(self,X):\n",
    "            return X[self.attribute_names].values\n",
    "    \n",
    "    #Attribute adder for data pre_processing - days creation\n",
    "    createdAt_ix, designApprovedDate_ix, applicationReceivedDate_ix, inspectionCompletedDate_ix, agreementEffectiveDate_ix  , ptoDate_ix , StatusDate_ix= [\n",
    "        list(pto_df_2019.columns).index(col)  \n",
    "        for col in (\"createdAt\",\"designApprovedDate\", \"applicationReceivedDate\", \"inspectionCompletedDate\", \"agreementEffectiveDate\",\"ptoDate\",\"StatusDate\")]\n",
    "\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "    def add_extra_features(X, compute_day_features=True):\n",
    "        if compute_day_features:\n",
    "            Days_created_to_application=(X['applicationReceivedDate']-X['createdAt']).dt.days\n",
    "            Days_application_to_design=(X['designApprovedDate']-X['applicationReceivedDate']).dt.days\n",
    "            Days_design_to_inspection=(X['inspectionCompletedDate']-X['designApprovedDate']).dt.days\n",
    "    #inspection completed - inspection submitted should be added\n",
    "            Days_inspection_to_agreement=(X['agreementEffectiveDate']-X['inspectionCompletedDate']).dt.days\n",
    "            Days_agreement_to_pto=(X['ptoDate']-X['agreementEffectiveDate']).dt.days\n",
    "\n",
    "            Days_recieved_to_status=(X['StatusDate']-X['applicationReceivedDate']).dt.days\n",
    "\n",
    "            Days_created_to_design=(X['designApprovedDate']-X['createdAt']).dt.days\n",
    "            Days_created_to_inspection=(X['inspectionCompletedDate']-X['createdAt']).dt.days\n",
    "            Days_created_to_agreement=(X['agreementEffectiveDate']-X['createdAt']).dt.days\n",
    "            Days_created_to_pto=(X['ptoDate']-X['createdAt']).dt.days\n",
    "            return np.c_[X,Days_created_to_application, Days_application_to_design, Days_design_to_inspection,\n",
    "                         Days_inspection_to_agreement,Days_agreement_to_pto,Days_recieved_to_status,Days_created_to_design,Days_created_to_inspection,Days_created_to_agreement,Days_created_to_pto]\n",
    "\n",
    "\n",
    "        else:\n",
    "            return np.c_[X]\n",
    "\n",
    "\n",
    "    attr_adder = FunctionTransformer(add_extra_features, validate=False,\n",
    "                                     kw_args={\"compute_day_features\": True})\n",
    "    \n",
    "\n",
    "\n",
    "    timeline_extra_attribs = attr_adder.fit_transform(pto_df_2019)\n",
    "    #timeline_extra_attribs = bias_adder.fit_transform(timeline_extra_attribs)\n",
    "    timeline_extra_attribs = pd.DataFrame(\n",
    "        timeline_extra_attribs,\n",
    "        columns=list(pto_df_2019.columns)+[\"Days_created_to_application\",\"Days_application_to_design\",\"Days_design_to_inspection\",\"Days_inspection_to_agreement\",\"Days_agreement_to_pto\",\"Days_recieved_to_status\",\"Days_created_to_design\",\"Days_created_to_inspection\",\"Days_created_to_agreement\",\"Days_created_to_pto\"],\n",
    "        index=pto_df_2019.index)\n",
    "    \n",
    "    timeline_targetimpute_temp = timeline_extra_attribs.dropna(subset=[leg])\n",
    "\n",
    "    def add_bias_features(X, compute_bias_features=True):\n",
    "        if compute_bias_features:\n",
    "            t=X\n",
    "            t=X.append(X.assign(**{leg: X[leg]+1 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]+2 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]+3 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]+4 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]+5 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]-1 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]-2 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]-3 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]-4 }), ignore_index=True)\n",
    "            t=t.append(X.assign(**{leg: X[leg]-5 }), ignore_index=True)\n",
    "            t = t[t[leg] > 0]  \n",
    "            X=t\n",
    "            return (X)\n",
    "        else:\n",
    "            return (X)\n",
    "\n",
    "    bias_adder = FunctionTransformer(add_bias_features, validate=False,kw_args={\"compute_bias_features\": bias})\n",
    "    timeline_targetimpute = bias_adder.fit_transform(timeline_targetimpute_temp)\n",
    "\n",
    "# add +/- 5 data target points to make the instances to 20k rows\n",
    "\n",
    "    #Testing and training set creation. \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_set, test_set = train_test_split(timeline_targetimpute, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Set label for train and test set \n",
    "    timeline = train_set.drop(leg, axis=1)\n",
    "    timeline_labels = train_set[leg].copy()\n",
    "    timeline_test = test_set.drop(leg, axis=1)\n",
    "    timeline_test_labels=test_set[leg].copy()\n",
    "\n",
    "    #NUMERICAL AND CATEGORICAL VARIABLE INITIALISATION - use initialisation with dtype\n",
    "    timeline_cat=timeline[cat]\n",
    "    timeline_num=timeline[num]\n",
    "\n",
    "\n",
    "    #Number and category pipeline creation\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "    num_attribs = list(timeline_num)\n",
    "    cat_attribs = list(timeline_cat)\n",
    "\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer',Imputer(strategy=\"mean\",axis=0)),\n",
    "        ('std_scaler',StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    cat_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('imputer',SimpleImputer(strategy=\"constant\",fill_value=\"Unknown\", verbose=0, copy=True, add_indicator=True)),\n",
    "        ('onehotencoder', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    #Feature union of different pipelines into full pipeline\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "    full_pipeline = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline),    \n",
    "    ])\n",
    "\n",
    "\n",
    "    timeline_prepared = full_pipeline.fit_transform(timeline)\n",
    "\n",
    "\n",
    "    # #Trying regression and prediction on some unseen test data\n",
    "    # attributes = num_attribs + cat_attribs\n",
    "    # some_data = timeline_test.iloc[:5]\n",
    "    # some_labels = timeline_test_labels.iloc[:5]\n",
    "    # some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "\n",
    "    #MODELS\n",
    "    #Random Forest (this is used to find important features too!)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    forest_reg = RandomForestRegressor(n_estimators=100, random_state=42, max_leaf_nodes=16, n_jobs=-1)\n",
    "    forest_reg.fit(timeline_prepared, timeline_labels)\n",
    "\n",
    "\n",
    "    #Linear SVR\n",
    "    from sklearn.svm import LinearSVR\n",
    "    svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\n",
    "    svm_reg1.fit(timeline_prepared, timeline_labels)\n",
    "\n",
    "\n",
    "    #SGD with lasso\n",
    "    from sklearn.linear_model import SGDRegressor\n",
    "    sgd_ridge_reg = SGDRegressor(max_iter=50, tol=-np.infty, penalty=\"l1\", random_state=42)\n",
    "    sgd_ridge_reg.fit(timeline_prepared,timeline_labels)\n",
    "\n",
    "\n",
    "    # from sklearn.model_selection import cross_val_score\n",
    "    # svm_scores = cross_val_score(svm_reg1, timeline_prepared, timeline_labels,scoring=\"r2\", cv=5)\n",
    "    # sgd_scores = cross_val_score(sgd_ridge_reg, timeline_prepared, timeline_labels,scoring=\"r2\", cv=5)\n",
    "    # forest_scores = cross_val_score(forest_reg, timeline_prepared, timeline_labels,scoring=\"r2\", cv=5)\n",
    "\n",
    "    # print(\"\\nforest\",forest_scores.mean())\n",
    "    # print(\"svm\",svm_scores.mean())\n",
    "    # print(\"sgd\",sgd_scores.mean())\n",
    "\n",
    "\n",
    "    #Save the model as a pickle in a file \n",
    "    \n",
    "    svmj=joblib.dump(svm_reg1, 'svm_model_leg1.pkl') \n",
    "    forj=joblib.dump(forest_reg, 'forest_model_leg1.pkl') \n",
    "    sgdj=joblib.dump(sgd_ridge_reg, 'sgd_model_leg1.pkl') \n",
    "\n",
    "    #filename='svm_model_leg1.pkl'\n",
    "    #svmj=pickle.dumps(svm_reg1,protocol=None, fix_imports=True)\n",
    "\n",
    "\n",
    "    #Categorical boolean mask\n",
    "    ohe=OneHotEncoder()\n",
    "    oh=ohe.fit_transform(timeline_cat)\n",
    "    ohej=joblib.dump(ohe.categories_, 'ohecategories_leg1.pkl') \n",
    "    #ohej=pickle.dumps(ohe.categories_,protocol=None, fix_imports=True) \n",
    "\n",
    "\n",
    "    return(svmj,ohej)\n",
    "\n",
    "\n",
    "def Predict(test_path):\n",
    "    location = test_path\n",
    "    from sklearn.externals import joblib \n",
    "    \n",
    "    #svmleg1= pickle.load(svmj)  \n",
    "    #ohecatvar1=pickle.load(ohej) \n",
    "\n",
    "    svmleg1= joblib.load('svm_model_leg1.pkl') \n",
    "    ohecatvar1=joblib.load('ohecategories_leg1.pkl') \n",
    "\n",
    "    df = pd.read_csv(location)\n",
    "    df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "    #Changing dtype\n",
    "    for col in [stringvar]:\n",
    "        df[col] = df[col].astype('str')\n",
    "\n",
    "    for col in [floatvar]:\n",
    "        df[col] = df[col].astype('float64')   \n",
    "\n",
    "    for col in [datetimevar]:\n",
    "        df[col] = df[col].astype('datetime64[ns]')   \n",
    "\n",
    "\n",
    "    timeline_cat=df[cat]\n",
    "    timeline_num=df[num]\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import Imputer\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    num_attribs = list(timeline_num)\n",
    "    cat_attribs = list(timeline_cat)\n",
    "\n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    class DataFrameSelector(BaseEstimator,TransformerMixin):\n",
    "        def __init__(self, attribute_names):\n",
    "            self.attribute_names = attribute_names\n",
    "        def fit(self, X, y=None):\n",
    "            return self\n",
    "        def transform(self,X):\n",
    "            return X[self.attribute_names].values\n",
    "\n",
    "    num_pipeline = Pipeline([\n",
    "        ('selector', DataFrameSelector(num_attribs)),\n",
    "        ('imputer',Imputer(strategy=\"mean\",axis=0)),\n",
    "        ('std_scaler',StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    cat_pipeline1 = Pipeline([\n",
    "        ('selector', DataFrameSelector(cat_attribs)),\n",
    "        ('imputer',SimpleImputer(strategy=\"constant\",fill_value=\"Unknown\", verbose=0, copy=True, add_indicator=True)),\n",
    "        ('onehotencoderleg', OneHotEncoder(categories=ohecatvar1,sparse=False, handle_unknown='ignore')),\n",
    "\n",
    "    ])\n",
    "\n",
    "\n",
    "    #Feature union of different pipelines into full pipeline\n",
    "    from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "    full_pipeline1 = FeatureUnion(transformer_list=[\n",
    "        (\"num_pipeline\", num_pipeline),\n",
    "        (\"cat_pipeline\", cat_pipeline1),    \n",
    "    ])\n",
    "\n",
    "\n",
    "    #leg1 predict df creation\n",
    "    timeline_prepared = full_pipeline1.fit_transform(df)\n",
    "    df_predict=svmleg1.predict(timeline_prepared)\n",
    "    # df_predict=pd.DataFrame(df_predict)\n",
    "    # df_predict=df_predict.rename(columns={0: \"Predicted_Result_Created_to_ApplicationReceived\"})\n",
    "    # df_concat = pd.concat([df,df_predict], axis=1)\n",
    "\n",
    "\n",
    "    # pd.DataFrame(df_concat).to_csv('Test_Results_Leg1.csv')\n",
    "    # timeline_prepared.shape\n",
    "    return (df_predict)\n",
    "    \n",
    "def Model_attributes():\n",
    "    attributes = cat + num\n",
    "    return (len(attributes),cat,num)\n",
    "    \n",
    "def Validate_features_with_database():\n",
    "    location = test_path    \n",
    "    df = pd.read_csv(location)\n",
    "    df = df.drop(df.columns[[0]], axis=1)\n",
    "    att = num + cat\n",
    "    result =  all(elem in df for elem in att)\n",
    "    match = list(set(att)-set(df))\n",
    "    return(result,match)   \n",
    "\n",
    "# #Trying regression and prediction on some unseen test data\n",
    "# attributes = num_attribs + cat_attribs\n",
    "# dff = pd.read_csv('TEST_SET_SEP30th.csv')\n",
    "# dff = dff.drop(dff.columns[[0]], axis=1)\n",
    "# for col in [stringvar]:\n",
    "#     dff[col] = dff[col].astype('str')\n",
    "    \n",
    "# for col in [floatvar]:\n",
    "#     dff[col] = dff[col].astype('float64')   \n",
    "    \n",
    "# for col in [datetimevar]:\n",
    "#     dff[col] = dff[col].astype('datetime64[ns]')  \n",
    "# some_data = dff.iloc[20:25]\n",
    "# some_data_prepared = full_pipeline.transform(some_data)\n",
    "\n",
    "# #print(\"Days \", list(some_data))\n",
    "# print(\"\\n\\nPred Forest:\", forest_reg.predict(some_data_prepared))\n",
    "# print(\"Pred SVM:\", svm_reg1.predict(some_data_prepared))\n",
    "# print(\"Pred SGD:\", sgd_ridge_reg.predict(some_data_prepared))\n",
    "# some_data.tlmMeterCnt\n",
    "\n",
    "# s = attr_adder.fit_transform(some_data)\n",
    "# s = pd.DataFrame(\n",
    "#     s,\n",
    "#     columns=list(some_data.columns)+[\"Days_created_to_application\",\"Days_application_to_design\",\"Days_design_to_inspection\",\"Days_inspection_to_agreement\",\"Days_agreement_to_pto\",\"Days_recieved_to_status\",\"Days_created_to_design\",\"Days_created_to_inspection\",\"Days_created_to_agreement\",\"Days_created_to_pto\"],\n",
    "#     index=some_data.index)\n",
    "\n",
    "# s = s.dropna(subset=[leg])\n",
    "# s[leg]\n",
    "# #s.tlmMeterCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, [])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'CNP_Live_Sep23rd.csv'\n",
    "test_path='TEST_SET_SEP30th.csv'\n",
    "\n",
    "#Train(path)\n",
    "#Predict(test_path)\n",
    "#Model_attributes()\n",
    "Validate_features_with_database()\n",
    "\n",
    "\n",
    "# file = open('svmimip','wb')\n",
    "# file2 = open('oheimp','wb')\n",
    "# file,file2=Train(path)\n",
    "\n",
    "#use pickle instead of joblib\n",
    "#map the name of the variables and attribute type (dict format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
